{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fname = 'data.npz'\n",
    "load_data = np.load(data_fname)\n",
    "true_pi = load_data['pi']\n",
    "true_mu = load_data['mu']\n",
    "samples = load_data['samples']\n",
    "adv_sample = load_data['adv_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.23705428, 0.06574439, 0.39978303, 0.07123911, 0.22617919]),\n",
       " array([[2.02889543, 0.58387321],\n",
       "        [1.48504397, 3.96130589],\n",
       "        [1.26997612, 1.46258317],\n",
       "        [3.4355634 , 1.58225174],\n",
       "        [1.47788008, 3.48312234]]),\n",
       " array([[ 4.17993725,  1.24175583],\n",
       "        [ 2.68098744,  0.32334823],\n",
       "        [ 1.41416657,  0.3940763 ],\n",
       "        [ 3.04086445,  4.25610681],\n",
       "        [ 1.87104121,  4.03256269],\n",
       "        [ 5.0214534 ,  1.57025624],\n",
       "        [ 3.1324006 ,  1.80930315],\n",
       "        [ 3.48725692,  2.19854219],\n",
       "        [ 2.67629033, -0.01613154],\n",
       "        [ 2.5919368 ,  2.00105177],\n",
       "        [-0.10403355,  0.37719824],\n",
       "        [ 3.65484623,  1.83349119],\n",
       "        [ 0.27127926,  2.74596415],\n",
       "        [ 1.16614525,  1.58661359],\n",
       "        [ 1.47989679,  0.89424985],\n",
       "        [ 0.9680544 ,  1.00286455],\n",
       "        [ 4.67228469,  2.75282497],\n",
       "        [ 0.3507703 ,  3.45548201],\n",
       "        [ 2.11184957,  2.31217297],\n",
       "        [ 2.10118491,  4.49285233]]),\n",
       " array([-1., -1.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pi, true_mu, samples[::50], adv_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MoG_prob(x, pi, mu, cov):\n",
    "    K, dim = mu.size()\n",
    "    assert x.size() == (dim,)\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    priors = torch.softmax(pi, dim=0)\n",
    "    \n",
    "    prob = 0.0\n",
    "    for k in range(K):\n",
    "        cov2 = torch.matmul(cov[k].t(), cov[k])\n",
    "        log_prob_k = -dim * 0.5 * math.log(2 * math.pi) - 0.5 * cov2.logdet() - 0.5 * cov2.inverse().matmul(x - mu[k]).dot(x - mu[k])\n",
    "        prob += torch.exp(log_prob_k) * priors[k]\n",
    "    return prob\n",
    "\n",
    "def MoG_loss(X, z, pi, mu, cov, lam):\n",
    "    # z: adv_sample\n",
    "    K, dim = mu.size()\n",
    "    assert X.size(1) == dim\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    loss = lam * MoG_prob(z, pi, mu, cov)\n",
    "    for x in X:\n",
    "        loss -= torch.log(MoG_prob(x, pi, mu, cov))\n",
    "    return loss\n",
    "\n",
    "def GD_solver(samples, adv_sample, lam=2.0, max_step=10000, lr=0.001, K=5, dim=2):\n",
    "    X = torch.FloatTensor(samples[:10])\n",
    "    z = torch.FloatTensor(adv_sample)\n",
    "\n",
    "    pi = torch.rand(K, dtype=torch.float)\n",
    "    pi /= pi.sum()\n",
    "    pi.requires_grad_()\n",
    "    mu = torch.randn(K, dim, dtype=torch.float)\n",
    "    mu.requires_grad_()\n",
    "    cov = torch.eye(dim, dtype=torch.float).repeat(K, 1, 1)\n",
    "    cov.requires_grad_()\n",
    "\n",
    "    params = [pi, mu, cov]\n",
    "    named_params = OrderedDict([('pi', pi), ('mu', mu), ('cov', cov)])\n",
    "\n",
    "    print('*** Init ***')\n",
    "    for n, p in named_params.items():\n",
    "        print(n)\n",
    "        print(p)\n",
    "        def _hook(grad, p=p, n=n):\n",
    "            if torch.isnan(grad).sum() > 0:\n",
    "                print('Error in grad:', grad)\n",
    "                print('Shape:', grad.size())\n",
    "                print('Parameter name:', n)\n",
    "                print('p.requires_grad:', p.requires_grad)\n",
    "                raise ValueError\n",
    "        p.register_hook(_hook)\n",
    "    print('loss:')\n",
    "    print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "\n",
    "    optimizer = optim.SGD(params, lr=lr)\n",
    "\n",
    "    for step in tqdm(range(max_step)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print('Step {}'.format(step + 1))\n",
    "            for n, p in named_params.items():\n",
    "                print(n)\n",
    "                print(p)\n",
    "            print('loss:')\n",
    "            print(loss)\n",
    "    \n",
    "    return pi, mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Init ***\n",
      "pi\n",
      "tensor([0.0526, 0.0831, 0.4028, 0.0535, 0.4080], requires_grad=True)\n",
      "mu\n",
      "tensor([[-0.0834,  1.5534],\n",
      "        [-1.1490,  0.1361],\n",
      "        [ 1.1723, -0.9099],\n",
      "        [-2.2406, -0.0429],\n",
      "        [-0.3928,  1.9011]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(6408.6011, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad13d148250d451e8e763b355138e81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10\n",
      "pi\n",
      "tensor([ 0.6713, -0.7028,  0.8155, -0.8786,  1.0946], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.9636,  2.0920],\n",
      "        [-0.8570,  0.2619],\n",
      "        [ 1.9601,  0.1706],\n",
      "        [-2.1903, -0.0256],\n",
      "        [ 0.8033,  2.6622]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[1.7688, 0.4196],\n",
      "         [0.3710, 1.5238]],\n",
      "\n",
      "        [[1.2724, 0.1534],\n",
      "         [0.1539, 0.9766]],\n",
      "\n",
      "        [[1.7543, 0.6018],\n",
      "         [0.6900, 1.3248]],\n",
      "\n",
      "        [[1.0841, 0.0351],\n",
      "         [0.0351, 1.0015]],\n",
      "\n",
      "        [[1.6517, 0.6080],\n",
      "         [0.4705, 1.7309]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(3951.1877, grad_fn=<ThSubBackward>)\n",
      "Step 20\n",
      "pi\n",
      "tensor([ 0.7717, -1.0206,  1.0013, -1.2517,  1.4992], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 1.4133,  2.2651],\n",
      "        [-0.7398,  0.3142],\n",
      "        [ 2.4006,  0.5993],\n",
      "        [-2.1628, -0.0161],\n",
      "        [ 1.4071,  2.9561]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 1.4023,  0.1861],\n",
      "         [ 0.1094,  1.4448]],\n",
      "\n",
      "        [[ 1.3342,  0.1946],\n",
      "         [ 0.1962,  0.9496]],\n",
      "\n",
      "        [[ 1.5258,  0.2663],\n",
      "         [ 0.4583,  1.0118]],\n",
      "\n",
      "        [[ 1.1217,  0.0522],\n",
      "         [ 0.0521,  1.0006]],\n",
      "\n",
      "        [[ 0.9688,  0.1671],\n",
      "         [-0.1057,  1.4228]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(3602.0115, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e4ca66d02b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoG_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lam=2.0\n",
    "max_step=100000\n",
    "lr=0.001\n",
    "K=5\n",
    "dim=2\n",
    "\n",
    "X = torch.FloatTensor(samples)\n",
    "z = torch.FloatTensor(adv_sample)\n",
    "\n",
    "pi = torch.rand(K, dtype=torch.float)\n",
    "pi /= pi.sum()\n",
    "pi.requires_grad_()\n",
    "mu = torch.randn(K, dim, dtype=torch.float)\n",
    "mu.requires_grad_()\n",
    "cov = torch.eye(dim, dtype=torch.float).repeat(K, 1, 1)\n",
    "cov.requires_grad_()\n",
    "\n",
    "params = [pi, mu, cov]\n",
    "named_params = OrderedDict([('pi', pi), ('mu', mu), ('cov', cov)])\n",
    "\n",
    "print('*** Init ***')\n",
    "for n, p in named_params.items():\n",
    "    print(n)\n",
    "    print(p)\n",
    "    def _hook(grad, p=p, n=n):\n",
    "        if torch.isnan(grad).sum() > 0:\n",
    "            print('Error in grad:', grad)\n",
    "            print('Shape:', grad.size())\n",
    "            print('Parameter name:', n)\n",
    "            print('p.requires_grad:', p.requires_grad)\n",
    "            raise ValueError\n",
    "    p.register_hook(_hook)\n",
    "print('loss:')\n",
    "print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "\n",
    "optimizer = optim.SGD(params, lr=lr)\n",
    "\n",
    "for step in tqdm(range(max_step)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print('Step {}'.format(step + 1))\n",
    "        for n, p in named_params.items():\n",
    "            print(n)\n",
    "            print(p)\n",
    "        print('loss:')\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Init ***\n",
      "pi\n",
      "tensor([0.1639, 0.1345, 0.0582, 0.4190, 0.2243], requires_grad=True)\n",
      "mu\n",
      "tensor([[-0.2219,  1.0180],\n",
      "        [-0.5286,  2.2717],\n",
      "        [-0.8652, -0.3107],\n",
      "        [-1.0530, -0.1700],\n",
      "        [-1.1053,  1.3982]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(69.5848, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e363755a854faea363508be5545df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000\n",
      "pi\n",
      "tensor([ 1.3371,  0.6631, -0.2347, -0.1623, -0.6032], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 1.9120,  1.3170],\n",
      "        [ 1.1485,  2.5273],\n",
      "        [-0.0710, -0.6226],\n",
      "        [-0.2550, -0.4544],\n",
      "        [-0.6992,  1.3855]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 1.7059, -0.2857],\n",
      "         [-0.0742,  0.3648]],\n",
      "\n",
      "        [[ 0.9451,  0.9877],\n",
      "         [ 0.5326,  0.8220]],\n",
      "\n",
      "        [[ 1.4542,  0.0374],\n",
      "         [-0.0307,  0.2930]],\n",
      "\n",
      "        [[ 1.5630, -0.0896],\n",
      "         [-0.1058,  0.4160]],\n",
      "\n",
      "        [[ 1.4418, -0.0182],\n",
      "         [-0.0026,  0.9869]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(29.4981, grad_fn=<ThSubBackward>)\n",
      "Step 2000\n",
      "pi\n",
      "tensor([ 1.4670,  0.8903,  0.1249, -0.4976, -0.9846], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 2.5388,  1.1934],\n",
      "        [ 1.5779,  3.0436],\n",
      "        [ 0.6267, -0.9487],\n",
      "        [ 0.0331, -0.7741],\n",
      "        [-0.5802,  1.3420]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 1.3027, -0.1987],\n",
      "         [-0.0402,  0.3528]],\n",
      "\n",
      "        [[ 0.6963,  0.6868],\n",
      "         [ 0.3562,  0.6063]],\n",
      "\n",
      "        [[ 1.0808,  0.6142],\n",
      "         [ 0.0330,  0.7490]],\n",
      "\n",
      "        [[ 1.3928,  0.6857],\n",
      "         [ 0.0251,  0.0104]],\n",
      "\n",
      "        [[ 1.5039, -0.0766],\n",
      "         [-0.0416,  0.9979]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(28.3801, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f75f7fa65e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGD_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a74cdc031b5e>\u001b[0m in \u001b[0;36mGD_solver\u001b[0;34m(samples, adv_sample, lam, max_step, lr, K, dim)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoG_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pi, mu, cov = GD_solver(samples, adv_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
