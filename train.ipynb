{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fname = 'data.npz'\n",
    "load_data = np.load(data_fname)\n",
    "true_pi = load_data['pi']\n",
    "true_mu = load_data['mu']\n",
    "samples = load_data['samples']\n",
    "adv_sample = load_data['adv_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.23705428, 0.06574439, 0.39978303, 0.07123911, 0.22617919]),\n",
       " array([[2.02889543, 0.58387321],\n",
       "        [1.48504397, 3.96130589],\n",
       "        [1.26997612, 1.46258317],\n",
       "        [3.4355634 , 1.58225174],\n",
       "        [1.47788008, 3.48312234]]),\n",
       " array([[ 4.17993725,  1.24175583],\n",
       "        [ 2.68098744,  0.32334823],\n",
       "        [ 1.41416657,  0.3940763 ],\n",
       "        [ 3.04086445,  4.25610681],\n",
       "        [ 1.87104121,  4.03256269],\n",
       "        [ 5.0214534 ,  1.57025624],\n",
       "        [ 3.1324006 ,  1.80930315],\n",
       "        [ 3.48725692,  2.19854219],\n",
       "        [ 2.67629033, -0.01613154],\n",
       "        [ 2.5919368 ,  2.00105177],\n",
       "        [-0.10403355,  0.37719824],\n",
       "        [ 3.65484623,  1.83349119],\n",
       "        [ 0.27127926,  2.74596415],\n",
       "        [ 1.16614525,  1.58661359],\n",
       "        [ 1.47989679,  0.89424985],\n",
       "        [ 0.9680544 ,  1.00286455],\n",
       "        [ 4.67228469,  2.75282497],\n",
       "        [ 0.3507703 ,  3.45548201],\n",
       "        [ 2.11184957,  2.31217297],\n",
       "        [ 2.10118491,  4.49285233]]),\n",
       " array([-1., -1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pi, true_mu, samples[::50], adv_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MoG_prob(x, pi, mu, cov):\n",
    "    K, dim = mu.size()\n",
    "    assert x.size() == (dim,)\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    prob = 0.0\n",
    "    for k in range(K):\n",
    "        log_prob_k = -dim * 0.5 * math.log(2 * math.pi) - 0.5 * cov[k].logdet() - 0.5 * cov[k].inverse().matmul(x - mu[k]).dot(x - mu[k])\n",
    "        prob += torch.exp(log_prob_k) * pi[k] / pi.sum()\n",
    "    return prob\n",
    "\n",
    "def MoG_loss(X, z, pi, mu, cov, lam):\n",
    "    # z: adv_sample\n",
    "    K, dim = mu.size()\n",
    "    assert X.size(1) == dim\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    loss = lam * MoG_prob(z, pi, mu, cov)\n",
    "    for x in X:\n",
    "        loss -= torch.log(MoG_prob(x, pi, mu, cov))\n",
    "    return loss\n",
    "\n",
    "def GD_solver(samples, adv_sample, lam=2.0, max_step=10000, lr=0.01, K=5, dim=2):\n",
    "    X = torch.tensor(samples)\n",
    "    z = torch.tensor(adv_sample)\n",
    "    \n",
    "    pi = torch.rand(K, requires_grad=True)\n",
    "    pi /= pi.sum()\n",
    "    mu = torch.randn(K, dim, requires_grad=True)\n",
    "    cov = torch.eye(dim).repeat(K, 1).requires_grad_()\n",
    "    \n",
    "    print('*** Init ***')\n",
    "    print('pi:')\n",
    "    print(pi)\n",
    "    print('mu:')\n",
    "    print(mu)\n",
    "    print('cov:')\n",
    "    print(cov)\n",
    "    print('loss:')\n",
    "    print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "    \n",
    "    optimizer = optim.SGD([pi, mu, cov], lr=lr)\n",
    "    \n",
    "    for step in tqdm(range(max_step)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % 100 == 0:\n",
    "            print('Step {}'.format(step))\n",
    "            print('pi:')\n",
    "            print(pi)\n",
    "            print('mu:')\n",
    "            print(mu)\n",
    "            print('cov:')\n",
    "            print(cov)\n",
    "            print('loss:')\n",
    "            print(loss)\n",
    "    \n",
    "    return pi, mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Init ***\n",
      "pi:\n",
      "tensor([0.0290, 0.2395, 0.2814, 0.1792, 0.2709], requires_grad=True)\n",
      "mu:\n",
      "tensor([[-0.2416,  0.1751],\n",
      "        [ 1.0049, -0.3738],\n",
      "        [-0.4084, -1.1477],\n",
      "        [ 0.4074,  1.6492],\n",
      "        [-0.0961,  1.1061]], requires_grad=True)\n",
      "cov:\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(51.8790, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21aef586759444bb0574d9320176344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99\n",
      "pi:\n",
      "tensor([nan, nan, nan, nan, nan], requires_grad=True)\n",
      "mu:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], requires_grad=True)\n",
      "cov:\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(nan, grad_fn=<ThSubBackward>)\n",
      "Step 199\n",
      "pi:\n",
      "tensor([nan, nan, nan, nan, nan], requires_grad=True)\n",
      "mu:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], requires_grad=True)\n",
      "cov:\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(nan, grad_fn=<ThSubBackward>)\n",
      "Step 299\n",
      "pi:\n",
      "tensor([nan, nan, nan, nan, nan], requires_grad=True)\n",
      "mu:\n",
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], requires_grad=True)\n",
      "cov:\n",
      "tensor([[[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]],\n",
      "\n",
      "        [[nan, nan],\n",
      "         [nan, nan]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(nan, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1d75e2aacc2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoG_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lam=2.0\n",
    "max_step=10000\n",
    "lr=0.01\n",
    "K=5\n",
    "dim=2\n",
    "\n",
    "X = torch.FloatTensor(samples[:10])\n",
    "z = torch.FloatTensor(adv_sample)\n",
    "\n",
    "pi = torch.rand(K, dtype=torch.float)\n",
    "pi /= pi.sum()\n",
    "pi.requires_grad_()\n",
    "mu = torch.randn(K, dim, dtype=torch.float)\n",
    "mu.requires_grad_()\n",
    "cov = torch.eye(dim, dtype=torch.float).repeat(K, 1, 1).requires_grad_()\n",
    "\n",
    "## @TODO: collect parameters & named parameters; register hook\n",
    "\n",
    "print('*** Init ***')\n",
    "print('pi:')\n",
    "print(pi)\n",
    "print('mu:')\n",
    "print(mu)\n",
    "print('cov:')\n",
    "print(cov)\n",
    "print('loss:')\n",
    "print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "\n",
    "optimizer = optim.SGD([pi, mu, cov], lr=lr)\n",
    "\n",
    "for step in tqdm(range(max_step)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print('Step {}'.format(step))\n",
    "        print('pi:')\n",
    "        print(pi)\n",
    "        print('mu:')\n",
    "        print(mu)\n",
    "        print('cov:')\n",
    "        print(cov)\n",
    "        print('loss:')\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
