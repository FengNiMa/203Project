{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fname = 'data.npz'\n",
    "load_data = np.load(data_fname)\n",
    "true_pi = load_data['pi']\n",
    "true_mu = load_data['mu']\n",
    "samples = load_data['samples']\n",
    "adv_sample = load_data['adv_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.23705428, 0.06574439, 0.39978303, 0.07123911, 0.22617919]),\n",
       " array([[2.02889543, 0.58387321],\n",
       "        [1.48504397, 3.96130589],\n",
       "        [1.26997612, 1.46258317],\n",
       "        [3.4355634 , 1.58225174],\n",
       "        [1.47788008, 3.48312234]]),\n",
       " array([[ 4.17993725,  1.24175583],\n",
       "        [ 2.68098744,  0.32334823],\n",
       "        [ 1.41416657,  0.3940763 ],\n",
       "        [ 3.04086445,  4.25610681],\n",
       "        [ 1.87104121,  4.03256269],\n",
       "        [ 5.0214534 ,  1.57025624],\n",
       "        [ 3.1324006 ,  1.80930315],\n",
       "        [ 3.48725692,  2.19854219],\n",
       "        [ 2.67629033, -0.01613154],\n",
       "        [ 2.5919368 ,  2.00105177],\n",
       "        [-0.10403355,  0.37719824],\n",
       "        [ 3.65484623,  1.83349119],\n",
       "        [ 0.27127926,  2.74596415],\n",
       "        [ 1.16614525,  1.58661359],\n",
       "        [ 1.47989679,  0.89424985],\n",
       "        [ 0.9680544 ,  1.00286455],\n",
       "        [ 4.67228469,  2.75282497],\n",
       "        [ 0.3507703 ,  3.45548201],\n",
       "        [ 2.11184957,  2.31217297],\n",
       "        [ 2.10118491,  4.49285233]]),\n",
       " array([-1., -1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pi, true_mu, samples[::50], adv_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MoG_prob(x, pi, mu, cov):\n",
    "    K, dim = mu.size()\n",
    "    assert x.size() == (dim,)\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    priors = torch.softmax(pi, dim=0)\n",
    "    \n",
    "    prob = 0.0\n",
    "    for k in range(K):\n",
    "        cov2 = torch.matmul(cov[k].t(), cov[k])\n",
    "        log_prob_k = -dim * 0.5 * math.log(2 * math.pi) - 0.5 * cov2.logdet() - 0.5 * cov2.inverse().matmul(x - mu[k]).dot(x - mu[k])\n",
    "        prob += torch.exp(log_prob_k) * priors[k]\n",
    "    return prob\n",
    "\n",
    "def MoG_loss(X, z, pi, mu, cov, lam):\n",
    "    # z: adv_sample\n",
    "    K, dim = mu.size()\n",
    "    assert X.size(1) == dim\n",
    "    assert pi.size() == (K,)\n",
    "    assert cov.size() == (K, dim, dim)\n",
    "    \n",
    "    loss = lam * MoG_prob(z, pi, mu, cov)\n",
    "    for x in X:\n",
    "        loss -= torch.log(MoG_prob(x, pi, mu, cov))\n",
    "    return loss\n",
    "\n",
    "def GD_solver(samples, adv_sample, lam=2.0, max_step=10000, lr=0.01, K=5, dim=2):\n",
    "    X = torch.tensor(samples)\n",
    "    z = torch.tensor(adv_sample)\n",
    "    \n",
    "    pi = torch.rand(K, requires_grad=True)\n",
    "    pi /= pi.sum()\n",
    "    mu = torch.randn(K, dim, requires_grad=True)\n",
    "    cov = torch.eye(dim).repeat(K, 1).requires_grad_()\n",
    "    \n",
    "    print('*** Init ***')\n",
    "    print('pi:')\n",
    "    print(pi)\n",
    "    print('mu:')\n",
    "    print(mu)\n",
    "    print('cov:')\n",
    "    print(cov)\n",
    "    print('loss:')\n",
    "    print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "    \n",
    "    optimizer = optim.SGD([pi, mu, cov], lr=lr)\n",
    "    \n",
    "    for step in tqdm(range(max_step)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % 100 == 0:\n",
    "            print('Step {}'.format(step))\n",
    "            print('pi:')\n",
    "            print(pi)\n",
    "            print('mu:')\n",
    "            print(mu)\n",
    "            print('cov:')\n",
    "            print(cov)\n",
    "            print('loss:')\n",
    "            print(loss)\n",
    "    \n",
    "    return pi, mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Init ***\n",
      "pi\n",
      "tensor([0.1777, 0.1601, 0.1980, 0.2296, 0.2346], requires_grad=True)\n",
      "mu\n",
      "tensor([[-0.6630, -0.7774],\n",
      "        [ 1.0629, -0.0470],\n",
      "        [-0.3393,  0.1417],\n",
      "        [-1.1408,  0.4482],\n",
      "        [ 2.0556, -2.8561]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(64.0133, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8159546da41349a2abb9e4850a3c789f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000\n",
      "pi\n",
      "tensor([-0.3527,  2.4673, -0.0929, -0.4501, -0.5716], requires_grad=True)\n",
      "mu\n",
      "tensor([[-0.2620, -0.7911],\n",
      "        [ 2.2233,  1.5458],\n",
      "        [ 0.2996,  0.3389],\n",
      "        [-0.8062,  0.5309],\n",
      "        [ 2.0056, -2.7227]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 1.0928,  0.0576],\n",
      "         [ 0.0689,  0.7590]],\n",
      "\n",
      "        [[ 1.1582, -0.0057],\n",
      "         [-0.0785,  1.3659]],\n",
      "\n",
      "        [[ 0.9570,  0.2624],\n",
      "         [ 0.2126,  1.2744]],\n",
      "\n",
      "        [[ 1.3312,  0.1374],\n",
      "         [ 0.1421,  1.1000]],\n",
      "\n",
      "        [[ 1.0128, -0.0693],\n",
      "         [-0.0744,  1.2059]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(34.4722, grad_fn=<ThSubBackward>)\n",
      "Step 2000\n",
      "pi\n",
      "tensor([-0.1258,  2.7062, -0.0869, -0.6749, -0.8186], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.2894, -1.0852],\n",
      "        [ 2.2727,  1.6652],\n",
      "        [ 0.6177,  0.3353],\n",
      "        [-0.7001,  0.5295],\n",
      "        [ 1.9796, -2.6588]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 0.6297,  0.6022],\n",
      "         [ 0.1295,  0.1141]],\n",
      "\n",
      "        [[ 1.1307, -0.0545],\n",
      "         [-0.1335,  1.3169]],\n",
      "\n",
      "        [[ 0.5083,  0.3505],\n",
      "         [ 0.2402,  1.2931]],\n",
      "\n",
      "        [[ 1.3894,  0.1349],\n",
      "         [ 0.1435,  1.1244]],\n",
      "\n",
      "        [[ 1.0095, -0.1030],\n",
      "         [-0.1089,  1.2753]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(31.2014, grad_fn=<ThSubBackward>)\n",
      "Step 3000\n",
      "pi\n",
      "tensor([-0.3293,  3.3455, -0.2787, -0.7879, -0.9496], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.3356, -0.9428],\n",
      "        [ 2.0565,  1.4224],\n",
      "        [ 0.6136,  0.3442],\n",
      "        [-0.6118,  0.4949],\n",
      "        [ 1.9339, -2.6001]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 0.6934,  0.2200],\n",
      "         [ 0.2707,  0.0814]],\n",
      "\n",
      "        [[ 1.2011,  0.1019],\n",
      "         [ 0.0434,  1.4415]],\n",
      "\n",
      "        [[ 0.0817,  0.4499],\n",
      "         [ 0.2283,  1.2720]],\n",
      "\n",
      "        [[ 1.4172,  0.1063],\n",
      "         [ 0.1196,  1.1637]],\n",
      "\n",
      "        [[ 1.0204, -0.1486],\n",
      "         [-0.1641,  1.3157]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(34.5777, grad_fn=<ThSubBackward>)\n",
      "Step 4000\n",
      "pi\n",
      "tensor([-0.4999,  3.8184, -0.4575, -0.8457, -1.0153], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.3356, -0.9428],\n",
      "        [ 2.0461,  1.4037],\n",
      "        [ 0.6136,  0.3442],\n",
      "        [-0.5695,  0.4783],\n",
      "        [ 1.9107, -2.5735]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 0.6934,  0.2200],\n",
      "         [ 0.2707,  0.0814]],\n",
      "\n",
      "        [[ 1.2018,  0.1051],\n",
      "         [ 0.0470,  1.4438]],\n",
      "\n",
      "        [[ 0.0817,  0.4499],\n",
      "         [ 0.2283,  1.2720]],\n",
      "\n",
      "        [[ 1.4262,  0.0952],\n",
      "         [ 0.1100,  1.1798]],\n",
      "\n",
      "        [[ 1.0243, -0.1703],\n",
      "         [-0.1907,  1.3293]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(34.2702, grad_fn=<ThSubBackward>)\n",
      "Step 5000\n",
      "pi\n",
      "tensor([-0.6045,  4.1129, -0.5663, -0.8839, -1.0582], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.3356, -0.9428],\n",
      "        [ 2.0435,  1.4004],\n",
      "        [ 0.6136,  0.3442],\n",
      "        [-0.5418,  0.4674],\n",
      "        [ 1.8954, -2.5560]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 0.6934,  0.2200],\n",
      "         [ 0.2707,  0.0814]],\n",
      "\n",
      "        [[ 1.2021,  0.1063],\n",
      "         [ 0.0484,  1.4448]],\n",
      "\n",
      "        [[ 0.0817,  0.4499],\n",
      "         [ 0.2283,  1.2720]],\n",
      "\n",
      "        [[ 1.4308,  0.0883],\n",
      "         [ 0.1041,  1.1892]],\n",
      "\n",
      "        [[ 1.0257, -0.1844],\n",
      "         [-0.2080,  1.3369]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(34.1538, grad_fn=<ThSubBackward>)\n",
      "Step 6000\n",
      "pi\n",
      "tensor([-0.6796,  4.3264, -0.6443, -0.9125, -1.0901], requires_grad=True)\n",
      "mu\n",
      "tensor([[ 0.3356, -0.9428],\n",
      "        [ 2.0422,  1.3987],\n",
      "        [ 0.6136,  0.3442],\n",
      "        [-0.5209,  0.4591],\n",
      "        [ 1.8837, -2.5427]], requires_grad=True)\n",
      "cov\n",
      "tensor([[[ 0.6934,  0.2200],\n",
      "         [ 0.2707,  0.0814]],\n",
      "\n",
      "        [[ 1.2022,  0.1070],\n",
      "         [ 0.0492,  1.4453]],\n",
      "\n",
      "        [[ 0.0817,  0.4499],\n",
      "         [ 0.2283,  1.2720]],\n",
      "\n",
      "        [[ 1.4336,  0.0834],\n",
      "         [ 0.0998,  1.1959]],\n",
      "\n",
      "        [[ 1.0262, -0.1949],\n",
      "         [-0.2209,  1.3421]]], requires_grad=True)\n",
      "loss:\n",
      "tensor(34.0930, grad_fn=<ThSubBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-1d846a6d1c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoG_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/syt/python3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lam=2.0\n",
    "max_step=100000\n",
    "lr=0.001\n",
    "K=5\n",
    "dim=2\n",
    "\n",
    "X = torch.FloatTensor(samples[:10])\n",
    "z = torch.FloatTensor(adv_sample)\n",
    "\n",
    "pi = torch.rand(K, dtype=torch.float)\n",
    "pi /= pi.sum()\n",
    "pi.requires_grad_()\n",
    "mu = torch.randn(K, dim, dtype=torch.float)\n",
    "mu.requires_grad_()\n",
    "cov = torch.eye(dim, dtype=torch.float).repeat(K, 1, 1)\n",
    "cov.requires_grad_()\n",
    "\n",
    "params = [pi, mu, cov]\n",
    "named_params = OrderedDict([('pi', pi), ('mu', mu), ('cov', cov)])\n",
    "\n",
    "print('*** Init ***')\n",
    "for n, p in named_params.items():\n",
    "    print(n)\n",
    "    print(p)\n",
    "    def _hook(grad, p=p, n=n):\n",
    "        if torch.isnan(grad).sum() > 0:\n",
    "            print('Error in grad:', grad)\n",
    "            print('Shape:', grad.size())\n",
    "            print('Parameter name:', n)\n",
    "            print('p.requires_grad:', p.requires_grad)\n",
    "            raise ValueError\n",
    "    p.register_hook(_hook)\n",
    "print('loss:')\n",
    "print(MoG_loss(X, z, pi, mu, cov, lam=lam))\n",
    "\n",
    "optimizer = optim.SGD(params, lr=lr)\n",
    "\n",
    "for step in tqdm(range(max_step)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = MoG_loss(X, z, pi, mu, cov, lam=lam)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step + 1) % 1000 == 0:\n",
    "        print('Step {}'.format(step + 1))\n",
    "        for n, p in named_params.items():\n",
    "            print(n)\n",
    "            print(p)\n",
    "        print('loss:')\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
